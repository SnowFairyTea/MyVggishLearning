{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f8a611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vggish/embedding:0\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "from importlib import import_module\n",
    "\n",
    "vggish_input=import_module(\".torchvggish.vggish_input\",\"torchvggish-master\")\n",
    "\n",
    "\n",
    "#GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "#損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4968bc0",
   "metadata": {},
   "source": [
    "# 今日は試しに学習させてみる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1398ebcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vggish/embedding:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Download an example audio file\\nimport urllib\\nurl, filename = (\"http://soundbible.com/grab.php?id=1698&type=wav\", \"bus_chatter.wav\")\\ntry: urllib.URLopener().retrieve(url, filename)\\nexcept: urllib.request.urlretrieve(url, filename)\\n\\nmodel.forward(filename)```'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('torchvggish-master', 'vggish', source='local', preprocess=False).to(device)\n",
    "#harritaylor/\n",
    "#model.eval()\n",
    "'''\n",
    "# Download an example audio file\n",
    "import urllib\n",
    "url, filename = (\"http://soundbible.com/grab.php?id=1698&type=wav\", \"bus_chatter.wav\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "model.forward(filename)```'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7201a52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyClassfilter(\n",
       "  (classfilter): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=64, out_features=5, bias=True)\n",
       "    (3): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyClassfilter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyClassfilter, self).__init__()\n",
    "        self.classfilter=nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64,5),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classfilter(x)\n",
    "mymodel = MyClassfilter().to(device)\n",
    "mymodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2adcdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGish(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (embeddings): Sequential(\n",
      "    (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (pproc): Postprocessor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 学習済みの重みを使用\n",
    "use_pretrained = True\n",
    "\n",
    "# モデルをロード\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2b9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('変更前 : ', model.embeddings[4])\n",
    "#model.embeddings[4] = nn.Linear(in_features=4096, out_features=5)\n",
    "#print('変更あと : ', model.embeddings[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8efc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.classfilter=nn.Sequential(\n",
    "#    nn.Linear(128, 5)   \n",
    "#)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4dc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#試しに実行\n",
    "#model.eval()# 評価しない\n",
    "import urllib\n",
    "url, filename = (\"http://soundbible.com/grab.php?id=1698&type=wav\", \"bus_chatter.wav\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "#a =model.forward(filename)\n",
    "#rint(a.size(),a.dim())\n",
    "#print(mymodel.forward(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390d55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データ数 :  162\n",
      "[['C:/Users/hase/git/youtubedl/result/Speech/--aE2O5G5WE.wav', 0], ['C:/Users/hase/git/youtubedl/result/Speech/--PJHxphWEs.wav', 0], ['C:/Users/hase/git/youtubedl/result/Speech/-30H9V1IKps.wav', 0]]\n",
      "検証データ数 :  44\n",
      "[['C:/Users/hase/git/youtubedl/result/Speech/-L44JqysUy8.wav', 0], ['C:/Users/hase/git/youtubedl/result/Speech/-L68XDzhn2g.wav', 0], ['C:/Users/hase/git/youtubedl/result/Speech/-LBl0UeKNyU.wav', 0]]\n"
     ]
    }
   ],
   "source": [
    "#データセットの定義？\n",
    "classes=[\"Speech\",\"Wind\",\"Animal\",\"Music\",\"Speech\"]\n",
    "dataset_dir=\"C:/Users/hase/git/youtubedl/result\"\n",
    "\n",
    "\n",
    "def make_path_list():\n",
    "    train_file_list=[]\n",
    "    valid_file_list=[]\n",
    "    for i in range(len(classes)):\n",
    "        dir_name=os.path.join(dataset_dir,classes[i]).replace(\"\\\\\",\"/\")\n",
    "        file_list=os.listdir(dir_name)\n",
    "        \n",
    "        #8割を学習用、残りを検証用にする\n",
    "        num_data = len(file_list)\n",
    "        num_split = int(num_data*0.8)\n",
    "        \n",
    "        train_file_list += [[os.path.join(dir_name, file).replace('\\\\', '/'), i] for file in file_list[:num_split] ]\n",
    "        valid_file_list += [[os.path.join(dir_name, file).replace('\\\\', '/'), i] for file in file_list[num_split:]]\n",
    "    return {\"train\":train_file_list,\"valid\":valid_file_list}\n",
    "# 画像データへのファイルパスとラベルを格納したリストを取得する\n",
    "data_dict = make_path_list()\n",
    "## リストが変かもなのでみてみる\n",
    "\n",
    "\n",
    "print('学習データ数 : ', len(data_dict[\"train\"]))\n",
    "##### 先頭3件だけ表示\n",
    "print(data_dict[\"train\"][:3])\n",
    "\n",
    "print('検証データ数 : ', len(data_dict[\"valid\"]))\n",
    "##### 先頭3件だけ表示\n",
    "print(data_dict[\"valid\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94164bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0830\n",
    "#DataSet型を作る必要がある気がする\n",
    "class MyDataset(data.Dataset):\n",
    "    '''\n",
    "    data_dictは[パス,番号]\n",
    "    '''\n",
    "    def __init__(self, _data_dict,  phase='train'):\n",
    "        #self.data_dict = data_dict\n",
    "        self.data_dict = []\n",
    "        for path, label in _data_dict:\n",
    "            for data in vggish_input.wavfile_to_examples(path):\n",
    "                self.data_dict.append([data, label])\n",
    "                \n",
    "        \n",
    "        self.phase = phase\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        wav_path,label = self.data_dict[index]\n",
    "        \n",
    "        return wav_path, label\n",
    "\n",
    "#DataSetを実際に作ってみる \n",
    "\n",
    "train_dataset = MyDataset(\n",
    "    _data_dict=data_dict[\"train\"],\n",
    "    phase=\"train\"\n",
    ")\n",
    "\n",
    "valid_dataset = MyDataset(\n",
    "    _data_dict=data_dict[\"valid\"],\n",
    "    phase=\"valid\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "599018e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-4.6052, -4.6052, -4.6052,  ..., -4.6052, -4.6052, -4.6052],\n",
      "         [-4.6052, -4.6052, -4.6052,  ..., -4.6052, -4.6052, -4.6052],\n",
      "         [-4.6047, -4.6047, -4.6046,  ..., -4.6019, -4.6023, -4.6025],\n",
      "         ...,\n",
      "         [ 2.8631,  2.4165,  1.0068,  ..., -1.3567, -0.7359, -0.9956],\n",
      "         [ 2.4454,  2.8380,  2.3015,  ..., -1.1201, -1.2774, -0.9788],\n",
      "         [ 2.8262,  2.5614,  1.9133,  ..., -1.2405, -1.3049, -1.1985]]],\n",
      "       grad_fn=<UnbindBackward>), 0) torch.Size([1, 96, 64])\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "tst=train_dataset.__getitem__(0)\n",
    "print(tst,tst[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f0dd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <torch.utils.data.dataloader.DataLoader object at 0x00000238239EB610>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x00000238239EBA30>}\n"
     ]
    }
   ],
   "source": [
    "#0830\n",
    "#もう1手ひつようらしい\n",
    "\n",
    "#dataloaderを用いてミニバッチを作成\n",
    "batch_size=10\n",
    "\n",
    "train_dataloader=data.DataLoader(\n",
    "    train_dataset, batch_size = batch_size, shuffle=True\n",
    ")\n",
    "valid_dataloader=data.DataLoader(\n",
    "    valid_dataset, batch_size = batch_size//2, shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_dict={\n",
    "    'train': train_dataloader, \n",
    "    'valid': valid_dataloader\n",
    "}\n",
    "print(dataloader_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b54849d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name :  features.0.weight\n",
      "name :  features.0.bias\n",
      "name :  features.3.weight\n",
      "name :  features.3.bias\n",
      "name :  features.6.weight\n",
      "name :  features.6.bias\n",
      "name :  features.8.weight\n",
      "name :  features.8.bias\n",
      "name :  features.11.weight\n",
      "name :  features.11.bias\n",
      "name :  features.13.weight\n",
      "name :  features.13.bias\n",
      "name :  embeddings.0.weight\n",
      "name :  embeddings.0.bias\n",
      "name :  embeddings.2.weight\n",
      "name :  embeddings.2.bias\n",
      "name :  embeddings.4.weight\n",
      "name :  embeddings.4.bias\n",
      "name :  pproc.pca_eigen_vectors\n",
      "name :  pproc.pca_means\n",
      "('classfilter.0.weight', Parameter containing:\n",
      "tensor([[ 6.5616e-02,  5.6828e-02,  7.1750e-02,  ..., -1.2929e-02,\n",
      "         -6.4636e-02, -8.6721e-02],\n",
      "        [ 5.3197e-05,  3.2198e-02,  3.7424e-02,  ...,  5.2885e-02,\n",
      "         -7.6512e-02, -3.9736e-02],\n",
      "        [-1.5083e-02,  1.7054e-02,  8.1912e-02,  ..., -9.5320e-03,\n",
      "         -7.7025e-03, -5.1590e-02],\n",
      "        ...,\n",
      "        [ 3.2359e-02,  8.7688e-02,  4.5022e-02,  ..., -2.2515e-02,\n",
      "          4.7380e-02, -2.8570e-02],\n",
      "        [-4.1647e-02,  6.3372e-02, -5.8904e-02,  ..., -7.2887e-02,\n",
      "          2.1344e-02, -6.1037e-02],\n",
      "        [-5.6292e-03,  3.9359e-02,  4.1075e-02,  ...,  7.1973e-02,\n",
      "          4.1850e-02,  4.5370e-02]], device='cuda:0', requires_grad=True))\n",
      "('classfilter.0.bias', Parameter containing:\n",
      "tensor([-0.0225, -0.0332, -0.0696,  0.0771, -0.0610,  0.0195, -0.0219, -0.0802,\n",
      "        -0.0818,  0.0069, -0.0687,  0.0308, -0.0384,  0.0763, -0.0027, -0.0687,\n",
      "        -0.0855,  0.0286, -0.0516,  0.0245, -0.0829, -0.0285,  0.0113,  0.0788,\n",
      "         0.0222,  0.0783, -0.0176, -0.0542, -0.0183,  0.0200, -0.0211,  0.0612,\n",
      "         0.0154, -0.0565,  0.0184,  0.0581,  0.0245, -0.0741,  0.0399,  0.0490,\n",
      "        -0.0006,  0.0151, -0.0261, -0.0416,  0.0463, -0.0686,  0.0394,  0.0368,\n",
      "        -0.0263, -0.0384, -0.0034, -0.0707, -0.0735, -0.0030, -0.0282, -0.0454,\n",
      "         0.0875,  0.0214, -0.0812,  0.0287,  0.0405,  0.0122, -0.0540, -0.0235],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('classfilter.2.weight', Parameter containing:\n",
      "tensor([[ 7.2957e-02, -8.2347e-02, -2.1742e-02, -1.1519e-05, -8.7340e-02,\n",
      "         -9.7082e-03, -5.3772e-02,  1.2034e-01, -2.3864e-02,  1.1139e-01,\n",
      "          5.4186e-02,  3.5822e-02, -1.1765e-01, -2.8552e-03,  1.1233e-01,\n",
      "         -2.9350e-02, -9.6291e-02, -7.3652e-02,  3.2380e-02, -9.2850e-02,\n",
      "          1.2946e-02,  1.1876e-01,  1.0428e-01,  1.1956e-01, -1.5838e-02,\n",
      "          4.5008e-02, -6.3463e-02,  1.0238e-01,  7.0727e-02, -7.5026e-02,\n",
      "         -1.1588e-01,  5.1454e-02,  1.2244e-01,  6.6927e-02,  3.9366e-02,\n",
      "         -4.5036e-02,  8.0346e-02, -4.6179e-02, -3.4156e-02, -8.0973e-02,\n",
      "         -1.0479e-01, -3.6539e-02,  3.1029e-02, -1.1124e-01, -6.1421e-03,\n",
      "         -1.2176e-01, -2.0624e-03, -6.1303e-02,  7.7140e-02, -1.1325e-01,\n",
      "          1.9166e-02, -3.2838e-02, -7.5438e-02,  3.0505e-02, -2.6222e-03,\n",
      "          8.6690e-02,  5.9078e-02,  5.8728e-02,  2.2673e-03, -5.4489e-02,\n",
      "          1.1453e-01,  3.4557e-02, -1.0548e-01,  4.6283e-02],\n",
      "        [ 2.8910e-02,  1.1443e-01, -7.7845e-02, -7.8863e-02, -8.3082e-02,\n",
      "          5.5906e-02,  1.0093e-01,  6.2789e-02, -1.2020e-01,  2.8835e-02,\n",
      "          6.5098e-02,  1.0032e-02,  2.6538e-02,  7.7006e-03,  5.5084e-02,\n",
      "         -4.3338e-02,  1.1219e-01,  4.3267e-02,  1.3626e-02, -4.3464e-03,\n",
      "          6.9009e-03,  4.7403e-02,  1.6529e-02,  9.0983e-02,  4.4053e-02,\n",
      "         -8.5243e-02, -7.4348e-02, -3.5524e-02,  5.0956e-02, -5.9935e-02,\n",
      "         -5.4047e-02, -2.8668e-02, -6.3974e-02, -8.4144e-02,  8.0040e-03,\n",
      "         -4.3251e-02, -5.5448e-03,  2.2114e-02, -7.0967e-02,  5.6006e-02,\n",
      "         -5.7194e-02, -9.2645e-02,  3.3752e-02, -7.6692e-02,  6.0723e-02,\n",
      "          9.3737e-02,  8.6873e-02, -8.7831e-02,  1.1813e-01,  1.1304e-01,\n",
      "          6.5648e-02,  9.9454e-03,  1.0803e-01, -7.3560e-02, -1.7780e-02,\n",
      "         -1.1184e-01,  1.0922e-01, -9.6300e-02,  1.2090e-01, -2.8586e-02,\n",
      "          3.6185e-02, -6.2275e-02,  1.1742e-01, -2.8364e-02],\n",
      "        [ 5.2653e-02, -7.9533e-02, -1.4478e-02,  7.5582e-02,  3.0409e-03,\n",
      "          9.8996e-02, -3.8082e-02,  4.2635e-02, -2.2482e-03,  1.7301e-02,\n",
      "         -1.2168e-01,  3.2827e-02, -5.5595e-02, -2.3613e-02,  6.7345e-02,\n",
      "         -4.3464e-02, -9.6116e-02,  8.2939e-02,  9.0255e-02, -1.2183e-01,\n",
      "          2.4450e-02,  1.9314e-02, -1.0503e-01,  1.1578e-01, -9.5565e-03,\n",
      "         -6.3121e-02,  1.1964e-01,  2.0800e-02, -6.6239e-02, -1.0091e-01,\n",
      "         -8.0459e-02, -7.8082e-02,  5.3396e-02, -1.5376e-02,  3.3676e-02,\n",
      "         -2.8272e-02, -7.4593e-02,  6.2150e-02,  2.8591e-02, -8.9014e-02,\n",
      "          9.1443e-02,  1.7233e-02, -6.5963e-02,  5.2887e-02, -4.6328e-02,\n",
      "          8.8987e-03, -3.1064e-02, -5.3214e-02, -8.5928e-02,  7.1944e-02,\n",
      "         -1.0080e-01,  3.7496e-02, -1.0877e-02, -1.0122e-01,  1.6911e-02,\n",
      "          9.8173e-02,  2.1628e-02, -8.8282e-02,  5.7251e-02, -4.6446e-02,\n",
      "          8.1593e-02,  4.9135e-02, -1.5510e-02,  7.1817e-02],\n",
      "        [ 1.1364e-01,  1.0159e-01, -1.0052e-01, -5.6832e-02,  1.0026e-01,\n",
      "         -7.6604e-02, -1.1348e-01,  1.1544e-01, -1.2141e-01,  1.0024e-01,\n",
      "         -5.7963e-02, -9.6285e-02, -1.0271e-01,  1.0628e-01, -2.1669e-02,\n",
      "          8.6278e-02, -5.0681e-02, -2.2810e-02,  1.1224e-01,  4.0143e-02,\n",
      "         -7.6351e-02,  2.4929e-02, -4.0659e-02,  5.4905e-02,  9.2921e-02,\n",
      "         -6.9988e-03, -4.0121e-03,  6.4361e-02,  1.1504e-01, -2.9430e-02,\n",
      "          5.7459e-02, -1.1384e-01,  9.0694e-02, -7.8303e-02,  1.0133e-01,\n",
      "          9.5917e-02, -6.7150e-02,  3.9252e-02,  2.1333e-02,  9.6445e-02,\n",
      "          8.9137e-02, -1.0481e-01, -1.0966e-01,  9.7808e-02,  7.4286e-02,\n",
      "         -7.5939e-02,  3.6738e-02,  7.9358e-02, -6.7585e-02,  2.3938e-02,\n",
      "         -4.0652e-02, -2.6141e-02, -2.5139e-02,  9.4904e-03,  1.2155e-01,\n",
      "         -1.0821e-01, -1.1451e-01, -5.0876e-02,  3.6169e-02, -1.1157e-01,\n",
      "         -6.6074e-02,  5.5097e-02, -6.8323e-02,  2.7723e-02],\n",
      "        [ 2.7712e-02, -3.9428e-02, -7.5018e-02, -7.0527e-02,  4.8997e-02,\n",
      "          7.8954e-02, -2.6628e-02,  2.5563e-02,  8.5683e-02, -1.0682e-01,\n",
      "         -5.8381e-03, -6.6579e-02,  1.1238e-01, -4.4137e-02, -8.8758e-02,\n",
      "         -6.6222e-02,  8.7791e-02, -9.6286e-02, -5.3722e-03,  9.4179e-02,\n",
      "          1.9693e-02,  9.0115e-02, -5.4369e-02, -9.2238e-03, -7.7813e-02,\n",
      "          1.0986e-01,  1.1002e-01, -1.0216e-01,  1.6208e-02,  8.4913e-03,\n",
      "         -9.1021e-02,  4.9665e-02, -2.2457e-02, -5.3939e-02, -5.6184e-02,\n",
      "          9.1029e-02, -1.0151e-01,  2.1956e-02, -6.6853e-02,  1.0809e-01,\n",
      "          1.1166e-01,  8.3295e-02,  9.1331e-02, -5.5809e-02, -9.3685e-03,\n",
      "          1.0500e-01,  1.0325e-01, -7.1780e-02,  1.1213e-02, -5.9976e-02,\n",
      "          7.4526e-02, -4.5174e-02, -5.2830e-02, -1.0299e-01, -9.9929e-02,\n",
      "         -9.1933e-02,  7.5005e-02, -2.0087e-04,  2.5844e-03,  2.3371e-02,\n",
      "          1.9022e-02, -3.0595e-02, -5.1381e-02, -6.2683e-02]], device='cuda:0',\n",
      "       requires_grad=True))\n",
      "('classfilter.2.bias', Parameter containing:\n",
      "tensor([ 0.1087, -0.0170,  0.0893,  0.1167, -0.0425], device='cuda:0',\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# パラメータ名の確認\n",
    "for name, param in model.named_parameters():\n",
    "    print('name : ', name)\n",
    "for prm in mymodel.named_parameters():\n",
    "    print(prm)\n",
    "#print(mymodel.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b35bef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name :  classfilter.0.weight\n",
      "name :  classfilter.0.bias\n",
      "name :  classfilter.2.weight\n"
     ]
    }
   ],
   "source": [
    "#学習させるパラメータを格納\n",
    "params_to_update=[]\n",
    "#学習させるパラメータ名\n",
    "update_param_names=['classfilter.0.weight', 'classfilter.0.bias', 'classfilter.2.weight']\n",
    "\n",
    "#対象以外は購買計算をせず、変化しないようにもする\n",
    "for name,param in mymodel.named_parameters():\n",
    "    if name in update_param_names:\n",
    "        param.requires_grad = True\n",
    "        params_to_update.append(param)\n",
    "        print(\"name : \",name)\n",
    "    else:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80179db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#optimizer = optim.SGD(params_to_update, lr=0.01)\n",
    "optimizer = optim.SGD(params_to_update, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e9128b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "train Loss: 16.0135 Acc: 0.2407\n",
      "valid Loss: 7.9769 Acc: 0.2528\n",
      "Epoch 2/20\n",
      "train Loss: 15.8836 Acc: 0.2886\n",
      "valid Loss: 7.9608 Acc: 0.2323\n",
      "Epoch 3/20\n",
      "train Loss: 15.7539 Acc: 0.2600\n",
      "valid Loss: 7.8947 Acc: 0.2916\n",
      "Epoch 4/20\n",
      "train Loss: 15.5667 Acc: 0.3638\n",
      "valid Loss: 7.8004 Acc: 0.2825\n",
      "Epoch 5/20\n",
      "train Loss: 15.3726 Acc: 0.3713\n",
      "valid Loss: 7.8019 Acc: 0.2597\n",
      "Epoch 6/20\n",
      "train Loss: 15.2257 Acc: 0.4229\n",
      "valid Loss: 7.7344 Acc: 0.3622\n",
      "Epoch 7/20\n",
      "train Loss: 15.0722 Acc: 0.4415\n",
      "valid Loss: 7.6456 Acc: 0.4032\n",
      "Epoch 8/20\n",
      "train Loss: 14.8936 Acc: 0.4677\n",
      "valid Loss: 7.5689 Acc: 0.3759\n",
      "Epoch 9/20\n",
      "train Loss: 14.7809 Acc: 0.4863\n",
      "valid Loss: 7.5361 Acc: 0.4191\n",
      "Epoch 10/20\n",
      "train Loss: 14.6547 Acc: 0.5100\n",
      "valid Loss: 7.4905 Acc: 0.4055\n",
      "Epoch 11/20\n",
      "train Loss: 14.5251 Acc: 0.5000\n",
      "valid Loss: 7.4481 Acc: 0.4647\n",
      "Epoch 12/20\n",
      "train Loss: 14.4628 Acc: 0.5187\n",
      "valid Loss: 7.3934 Acc: 0.4579\n",
      "Epoch 13/20\n",
      "train Loss: 14.2824 Acc: 0.5336\n",
      "valid Loss: 7.4353 Acc: 0.4396\n",
      "Epoch 14/20\n",
      "train Loss: 14.1687 Acc: 0.5404\n",
      "valid Loss: 7.3358 Acc: 0.4920\n",
      "Epoch 15/20\n",
      "train Loss: 14.0495 Acc: 0.5560\n",
      "valid Loss: 7.3694 Acc: 0.4510\n",
      "Epoch 16/20\n",
      "train Loss: 14.0078 Acc: 0.5678\n",
      "valid Loss: 7.2754 Acc: 0.4647\n",
      "Epoch 17/20\n",
      "train Loss: 13.8881 Acc: 0.5684\n",
      "valid Loss: 7.3019 Acc: 0.4806\n",
      "Epoch 18/20\n",
      "train Loss: 13.7485 Acc: 0.5678\n",
      "valid Loss: 7.2504 Acc: 0.4510\n",
      "Epoch 19/20\n",
      "train Loss: 13.6745 Acc: 0.5765\n",
      "valid Loss: 7.2728 Acc: 0.4556\n",
      "Epoch 20/20\n",
      "train Loss: 13.7418 Acc: 0.5877\n",
      "valid Loss: 7.2358 Acc: 0.4579\n"
     ]
    }
   ],
   "source": [
    "#エポック数\n",
    "num_epochs=20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch+1,num_epochs))\n",
    "    \n",
    "    for phase in ['train','valid']:\n",
    "        if phase == 'train':\n",
    "            mymodel.train()#学習モード\n",
    "        else:\n",
    "            mymodel.eval()#検証？\n",
    "    \n",
    "        #epoch全体の損失の輪と正解数\n",
    "        epoch_loss=0.0\n",
    "        epoch_corrects=0\n",
    "        \n",
    "        count=0.0\n",
    "        #print(phase)\n",
    "        for inputs, labels in dataloader_dict[phase]:\n",
    "            #入力の確認\n",
    "            #print(len(inputs),len(labels))\n",
    "            #optimizer?を初期化?する?\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "\n",
    "                #outputs=mymodel(torch.tensor([model(input).tolist()[0] for input in inputs]).to(device))\n",
    "               #0906 これ([0])はあかんやろってなった\n",
    "                #for i, label in zip(inputs, labels):\n",
    "                out1=model(inputs)\n",
    "                outputs=mymodel(out1)\n",
    "                    #print(output)\n",
    "                    #for h in output:\n",
    "                       # print(h.tolist())\n",
    "                        #output_models.append(h.tolist())\n",
    "                        #output_models.append(h)\n",
    "                        #output_labels.append(label.tolist())\n",
    "                        #count+=1\n",
    "                        #print(label)\n",
    "                \n",
    "                #output_models=torch.tensor(output_models, device=device, grad_fn=output.grad_fn)\n",
    "                #output_labels=torch.tensor(output_labels, device=device)\n",
    "                labels=labels.to(device)\n",
    "                #print(output_models)\n",
    "                #loss=criterion(output_models, output_labels)\n",
    "                loss=criterion(outputs, labels)\n",
    "                #ラベルを予測\n",
    "                _,preds = torch.max(outputs,1)\n",
    "                \n",
    "                #訓練時は逆伝搬の計算\n",
    "                if phase == \"train\":\n",
    "                    #逆伝搬\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    #パラメータ更新\n",
    "                    optimizer.step()\n",
    "                \n",
    "                #イテレーション結果の計算\n",
    "                #lossの合計を更新\n",
    "                #pytorchの使用上バッチ内の平均lossが計算されているのでデータ数をかけて合計にする\n",
    "                #損失和を「全データの損失/データ数」で求めるせいらしい?\n",
    "                #print(len(inputs))\n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                #epoch_loss += loss.item()*output_labels.size(0)\n",
    "                \n",
    "                #正解数の合計を更新\n",
    "                epoch_corrects += torch.sum(preds == labels.data)\n",
    "                #print(preds.size(),count,output_labels.size(0))\n",
    "                #print()\n",
    "                #epoch_corrects += torch.sum(preds == output_labels)\n",
    "        #epochのlossと正解数の表示\n",
    "        epoch_loss=epoch_loss/len(dataloader_dict[phase])\n",
    "        epoch_acc=epoch_corrects.double()/len(dataloader_dict[phase].dataset)\n",
    "        #print(epoch_loss, epoch_corrects.double(), count)\n",
    "        #epoch_loss=epoch_loss/count\n",
    "        #epoch_acc=epoch_corrects.double()/count\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "m = nn.Softmax(dim=2)\n",
    "input = torch.randn(4, 4)\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0db113",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "input = torch.randn(2)\n",
    "output = m(input)\n",
    "print(input,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(\"C:/Users/hase/git/youtubedl/result/Speech/--aE2O5G5WE.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562ad1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b45a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(input,target,output, sep='\\n')\n",
    "print(output.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ee9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst= vggish_input.wavfile_to_examples(\"C:/Users/hase/git/youtubedl/result/Speech/--aE2O5G5WE.wav\")\n",
    "print(tst.size())\n",
    "for i in tst:\n",
    "    print(i.size())\n",
    "    print (model(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ef847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013da01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2c9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11bcca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed3a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
